{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Steps Involved in Building Facial Landmarks Detection Application**\n",
        "1.   Importing all the essential libraries\n",
        "2.   ListReading a sample image\n",
        "3. Performing facial landmarks detection and printing the  result\n",
        "4. Drawing the results on the sample image item\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U5ptSyfYww96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Import the Libraries**"
      ],
      "metadata": {
        "id": "4TiZ_IRvxH5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import itertools\n",
        "import numpy as np\n",
        "from time import time\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-oqPTKQ7xT45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 . ListReading a sample image**\n",
        "\n",
        "\n",
        "*   **initialization of the Facial landmarks detection model using the face_mesh method of Mediapipe’s solutions class. In the face_mesh method, we have our main function which will perform the landmarks detection which is FaceMesh()**\n",
        "\n",
        "*   **static_image_mode: This argument takes only two boolean types of values which are either True or False. If we will set the argument as False then it will detect the frames and not in the manner of images i.e. in the format of video streaming which can be helpful when we will try to detect the facial landmarks in real-time but in this case we want to detect the landmarks in the images so for that reason we will be using the True as the parameter. The default value is False.**\n",
        "\n",
        "*   **max_num_faces: This argument will specify how many faces the model should detect in the image or frame at one particular instance i.e in one frame/image the model can detect more than one face, it takes integer type of value i.e. numeric value and the default value is 1 (only one face to be detected).**\n",
        "\n",
        "*   **min_detection_confidence: This argument is basically for setting up the threshold value for the confidence of detection of the model i.e how much our model is confident that in the image/frame it has detected all the landmarks position. The typical range of values it takes is [0.0,1.0] and the default value is 0.5 i.e the confidence of detection is set to 50% confidence.**\n",
        "\n",
        "*   **min_tracking_confidence: This argument will be ignored when the static_image_mode parameter is set to True which is the case for images the default value of this argument is 0.5 i.e 50% confidence of tracking the confidence which also improves the robustness of the detection process.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8WYM2axxRqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "face_mesh_images = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=2,\n",
        "                                         min_detection_confidence=0.5)\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "mp_drawing_styles = mp.solutions.drawing_styles"
      ],
      "metadata": {
        "id": "OOtD-odjy4Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read an Image**"
      ],
      "metadata": {
        "id": "2t3pMz04z0b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img = cv2.imread('media/sample.jpg')\n",
        "\n",
        "plt.figure(figsize = [10, 10])\n",
        "\n",
        "plt.title(\"Sample Image\");plt.axis('off');plt.imshow(sample_img[:,:,::-1]);plt.show()"
      ],
      "metadata": {
        "id": "uXse_0jNzzmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Landmark Extraction**\n",
        "\n",
        "* Perform the face landmarks detection using process the function but before that, we will be converting the color format of the image from BGR to RGB.\n",
        "\n",
        "* Store the left eye and right eye coordinate values in separate variables using the iter tools chain method.\n",
        "\n",
        "* Before heading towards the main functionality it’s always a better idea to do some validations for that we will first check whether we have found the faces or not.\n",
        "\n",
        "* So, now only if the above statement is correct then only we will iterate over all the faces depending on the arguments.\n",
        "Then we will display the face number upon successfully detecting all the faces.\n",
        "\n",
        "* As of now, we are looking for the landmarks points of the left eye and right eye so we will display the name of them too.\n",
        "\n",
        "* First, we will iterate over two landmarks points of the left eye.\n",
        "Then we will do the same for the right eye using for loop.\n",
        "\n",
        "* Then finally we will display the landmarks points but those indexes will be in the normalized form so that the ML pipeline behind this detection can give equal weightage to each point."
      ],
      "metadata": {
        "id": "5O_NpO3Z0GzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "face_mesh_results = face_mesh_images.process(sample_img[:,:,::-1])\n",
        "\n",
        "LEFT_EYE_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\n",
        "RIGHT_EYE_INDEXES = list(set(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))\n",
        "\n",
        "if face_mesh_results.multi_face_landmarks:\n",
        "\n",
        "    for face_no, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
        "\n",
        "        print(f'FACE NUMBER: {face_no+1}')\n",
        "        print('-----------------------')\n",
        "\n",
        "        print(f'LEFT EYE LANDMARKS:n')\n",
        "\n",
        "        for LEFT_EYE_INDEX in LEFT_EYE_INDEXES[:2]:\n",
        "\n",
        "            print(face_landmarks.landmark[LEFT_EYE_INDEX])\n",
        "\n",
        "        print(f'RIGHT EYE LANDMARKS:n')\n",
        "\n",
        "        for RIGHT_EYE_INDEX in RIGHT_EYE_INDEXES[:2]:\n",
        "\n",
        "            print(face_landmarks.landmark[RIGHT_EYE_INDEX])"
      ],
      "metadata": {
        "id": "dxZWnnmO0E9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eICgamEc2Gp9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vgk0hGjP2Fro"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}